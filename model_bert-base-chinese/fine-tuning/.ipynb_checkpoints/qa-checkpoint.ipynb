{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fcee39e-ae9b-4ca3-8c9d-47f493697bfc",
   "metadata": {},
   "source": [
    "# [基于BERT预训练模型的SQuAD问答任务](https://www.ylkz.life/deeplearning/p10265968/)\n",
    "# [基于 Bert 的中文问答机器人](https://fengchao.pro/blog/bert-qa/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d5c21a7-9856-4aba-973f-43b6e2e18782",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import pandas as pd\n",
    "from torch.optim import AdamW\n",
    "from transformers import BertTokenizerFast, BertForQuestionAnswering\n",
    "\n",
    "tran_file_path = 'dataset/cmrc2018/train.json'\n",
    "test_file_path = 'dataset/cmrc2018/test.json'\n",
    "dev_file_path = 'dataset/cmrc2018/dev.json'\n",
    "trial_file_path = 'dataset/cmrc2018/trial.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b4a61a-3e25-4fe5-ba46-c559c0d2ee9b",
   "metadata": {},
   "source": [
    "## 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afd5af35-489e-4863-8f3a-24b210ae731e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device= cuda\n"
     ]
    }
   ],
   "source": [
    "# 优先使用 GPU\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('device=', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1565eeaf-c345-4e67-a992-366727bd8697",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForQuestionAnswering: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForQuestionAnswering(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载预训练模型\n",
    "model = BertForQuestionAnswering.from_pretrained('bert-base-chinese')\n",
    "# local_model = '/root/.cache/huggingface/hub/models--bert-base-chinese/snapshots/c30a6ed22ab4564dc1e3b2ecbf6e766b0611a33f'\n",
    "# model = BertForQuestionAnswering.from_pretrained(local_model)\n",
    "# 需要移动到cuda上\n",
    "model.to(device)\n",
    "\n",
    "# 不训练,不需要计算梯度\n",
    "# for param in model.parameters():\n",
    "#     param.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16e9c55f-5942-496d-83db-3a6a75623ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-chinese\")\n",
    "# tokenizer = BertTokenizerFast.from_pretrained(local_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df5e6d0-4363-40fa-a0be-8bdefa75db41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "71a5ee0f-f2f1-4df8-9954-38ab24883352",
   "metadata": {},
   "source": [
    "## 数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c8da96-3c3c-410a-a9b8-1edc5dc93de7",
   "metadata": {},
   "source": [
    "#### 数据集样例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "201e081e-3a09-46dc-86d6-5a027d7c34ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'paragraphs': [{'id': 'TRAIN_186',\n",
       "   'context': '范廷颂枢机（，），圣名保禄·若瑟（），是越南罗马天主教枢机。1963年被任为主教；1990年被擢升为天主教河内总教区宗座署理；1994年被擢升为总主教，同年年底被擢升为枢机；2009年2月离世。范廷颂于1919年6月15日在越南宁平省天主教发艳教区出生；童年时接受良好教育后，被一位越南神父带到河内继续其学业。范廷颂于1940年在河内大修道院完成神学学业。范廷颂于1949年6月6日在河内的主教座堂晋铎；及后被派到圣女小德兰孤儿院服务。1950年代，范廷颂在河内堂区创建移民接待中心以收容到河内避战的难民。1954年，法越战争结束，越南民主共和国建都河内，当时很多天主教神职人员逃至越南的南方，但范廷颂仍然留在河内。翌年管理圣若望小修院；惟在1960年因捍卫修院的自由、自治及拒绝政府在修院设政治课的要求而被捕。1963年4月5日，教宗任命范廷颂为天主教北宁教区主教，同年8月15日就任；其牧铭为「我信天主的爱」。由于范廷颂被越南政府软禁差不多30年，因此他无法到所属堂区进行牧灵工作而专注研读等工作。范廷颂除了面对战争、贫困、被当局迫害天主教会等问题外，也秘密恢复修院、创建女修会团体等。1990年，教宗若望保禄二世在同年6月18日擢升范廷颂为天主教河内总教区宗座署理以填补该教区总主教的空缺。1994年3月23日，范廷颂被教宗若望保禄二世擢升为天主教河内总教区总主教并兼天主教谅山教区宗座署理；同年11月26日，若望保禄二世擢升范廷颂为枢机。范廷颂在1995年至2001年期间出任天主教越南主教团主席。2003年4月26日，教宗若望保禄二世任命天主教谅山教区兼天主教高平教区吴光杰主教为天主教河内总教区署理主教；及至2005年2月19日，范廷颂因获批辞去总主教职务而荣休；吴光杰同日真除天主教河内总教区总主教职务。范廷颂于2009年2月22日清晨在河内离世，享年89岁；其葬礼于同月26日上午在天主教河内总教区总主教座堂举行。',\n",
       "   'qas': [{'question': '范廷颂是什么时候被任为主教的？',\n",
       "     'id': 'TRAIN_186_QUERY_0',\n",
       "     'answers': [{'text': '1963年', 'answer_start': 30}]},\n",
       "    {'question': '1990年，范廷颂担任什么职务？',\n",
       "     'id': 'TRAIN_186_QUERY_1',\n",
       "     'answers': [{'text': '1990年被擢升为天主教河内总教区宗座署理', 'answer_start': 41}]},\n",
       "    {'question': '范廷颂是于何时何地出生的？',\n",
       "     'id': 'TRAIN_186_QUERY_2',\n",
       "     'answers': [{'text': '范廷颂于1919年6月15日在越南宁平省天主教发艳教区出生',\n",
       "       'answer_start': 97}]},\n",
       "    {'question': '1994年3月，范廷颂担任什么职务？',\n",
       "     'id': 'TRAIN_186_QUERY_3',\n",
       "     'answers': [{'text': '1994年3月23日，范廷颂被教宗若望保禄二世擢升为天主教河内总教区总主教并兼天主教谅山教区宗座署理',\n",
       "       'answer_start': 548}]},\n",
       "    {'question': '范廷颂是何时去世的？',\n",
       "     'id': 'TRAIN_186_QUERY_4',\n",
       "     'answers': [{'text': '范廷颂于2009年2月22日清晨在河内离世', 'answer_start': 759}]}]}],\n",
       " 'id': 'TRAIN_186',\n",
       " 'title': '范廷颂'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# /root/workspace/zero_ai/model_bert-base-chinese/fine-tuning/dataset/cmrc2018/train.json\n",
    "with open(tran_file_path, 'r+', encoding='utf-8') as tran_file:\n",
    "     tran_sample = json.load(tran_file)\n",
    "tran_sample['data'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef96d08-73fc-47ef-969e-72f48e95f1a5",
   "metadata": {},
   "source": [
    "#### 重构样本\n",
    "> 因为 input_ids 的长度为 512，如果【问题】+【上下文】的长度超过此长度，无法使用。<br/>\n",
    "> 故采用滑动窗口处理样本、训练，预测时也采用滑动窗口方式预测。如下图：<br/>\n",
    "\n",
    "<img src=\"https://moonhotel.oss-cn-shanghai.aliyuncs.com/images/22010353470.jpg\" style=\"width: 400px;\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1858881-4578-440a-8a4a-76f4d0017e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 101, 5745, 2455, 7563, 3221,  784,  720, 3198,  952, 6158,  818,  711,\n",
      "         712, 3136, 4638, 8043,  102, 5745, 2455, 7563, 3364, 3322, 8020, 8024,\n",
      "        8021, 8024, 1760, 1399,  924, 4882,  185, 5735, 4449, 8020, 8021, 8024,\n",
      "        3221, 6632, 1298, 5384, 7716, 1921,  712, 3136, 3364, 3322,  511, 9155,\n",
      "        2399, 6158,  818,  711,  712, 3136, 8039, 8431, 2399, 6158, 3091, 1285,\n",
      "         711, 1921,  712, 3136, 3777, 1079, 2600, 3136, 1277, 2134, 2429, 5392,\n",
      "        4415, 8039, 8447, 2399, 6158, 3091, 1285,  711, 2600,  712, 3136, 8024,\n",
      "        1398, 2399, 2399, 2419, 6158, 3091, 1285,  711, 3364, 3322, 8039, 8170,\n",
      "        2399,  123, 3299, 4895,  686,  511, 5745, 2455, 7563,  754, 9915, 2399,\n",
      "         127, 3299, 8115, 3189, 1762, 6632, 1298, 2123, 2398, 4689, 1921,  712,\n",
      "        3136, 1355, 5683, 3136, 1277, 1139, 4495, 8039, 4997, 2399, 3198, 2970,\n",
      "        1358, 5679, 1962, 3136, 5509, 1400, 8024, 6158,  671,  855, 6632, 1298,\n",
      "        4868, 4266, 2372, 1168, 3777, 1079, 5326, 5330, 1071, 2110,  689,  511,\n",
      "        5745, 2455, 7563,  754, 9211, 2399, 1762, 3777, 1079, 1920,  934, 6887,\n",
      "        7368, 2130, 2768, 4868, 2110, 2110,  689,  511, 5745, 2455, 7563,  754,\n",
      "        8594, 2399,  127, 3299,  127, 3189, 1762, 3777, 1079, 4638,  712, 3136,\n",
      "        2429, 1828, 3232, 7195, 8039, 1350, 1400, 6158, 3836, 1168, 1760, 1957,\n",
      "        2207, 2548, 1065, 2109, 1036, 7368, 3302, 1218,  511, 8707, 2399,  807,\n",
      "        8024, 5745, 2455, 7563, 1762, 3777, 1079, 1828, 1277, 1158, 2456, 4919,\n",
      "        3696, 2970, 2521,  704, 2552,  809, 3119, 2159, 1168, 3777, 1079, 6912,\n",
      "        2773, 4638, 7410, 3696,  511, 9258, 2399, 8024, 3791, 6632, 2773,  751,\n",
      "        5310, 3338, 8024, 6632, 1298, 3696,  712, 1066, 1469, 1744, 2456, 6963,\n",
      "        3777, 1079, 8024, 2496, 3198, 2523, 1914, 1921,  712, 3136, 4868, 5466,\n",
      "         782, 1447, 6845, 5635, 6632, 1298, 4638, 1298, 3175, 8024,  852, 5745,\n",
      "        2455, 7563,  793, 4197, 4522, 1762, 3777, 1079,  511, 5422, 2399, 5052,\n",
      "        4415, 1760, 5735, 3307, 2207,  934, 7368, 8039, 2668, 1762, 8779, 2399,\n",
      "        1728, 2932, 1310,  934, 7368, 4638, 5632, 4507,  510, 5632, 3780, 1350,\n",
      "        2867, 5318, 3124, 2424, 1762,  934, 7368, 6392, 3124, 3780, 6440, 4638,\n",
      "        6206, 3724, 5445, 6158, 2936,  511, 9155, 2399,  125, 3299,  126, 3189,\n",
      "        8024, 3136, 2134,  818, 1462, 5745, 2455, 7563,  711, 1921,  712, 3136,\n",
      "        1266, 2123, 3136, 1277,  712, 3136, 8024, 1398, 2399,  129, 3299, 8115,\n",
      "        3189, 2218,  818, 8039, 1071, 4288, 7208,  711,  519, 2769,  928, 1921,\n",
      "         712, 4638, 4263,  520,  511, 4507,  754, 5745, 2455, 7563, 6158, 6632,\n",
      "        1298, 3124, 2424, 6763, 4881, 2345,  679, 1914, 8114, 2399, 8024, 1728,\n",
      "        3634,  800, 3187, 3791, 1168, 2792, 2247, 1828, 1277, 6822, 6121, 4288,\n",
      "        4130, 2339,  868, 5445,  683, 3800, 4777, 6438, 5023, 2339,  868,  511,\n",
      "        5745, 2455, 7563, 7370,  749, 7481, 2190, 2773,  751,  510, 6577, 1737,\n",
      "         510, 6158, 2496, 2229, 6833, 2154, 1921,  712, 3136,  833, 5023, 7309,\n",
      "        7579, 1912, 8024,  738, 4908, 2166, 2612, 1908,  934, 7368,  510, 1158,\n",
      "        2456, 1957,  934,  833, 1730,  860, 5023,  511, 8431, 2399, 8024, 3136,\n",
      "        2134, 5735, 3307,  924, 4882,  753,  686, 1762, 1398, 2399,  127, 3299,\n",
      "        8123, 3189, 3091, 1285, 5745, 2455, 7563,  711, 1921,  712, 3136, 3777,\n",
      "        1079, 2600, 3136, 1277, 2134, 2429, 5392,  102])\n",
      "[CLS] 范 廷 颂 是 什 么 时 候 被 任 为 主 教 的 ？ [SEP] 范 廷 颂 枢 机 （ ， ） ， 圣 名 保 禄 · 若 瑟 （ ） ， 是 越 南 罗 马 天 主 教 枢 机 。 1963 年 被 任 为 主 教 ； 1990 年 被 擢 升 为 天 主 教 河 内 总 教 区 宗 座 署 理 ； 1994 年 被 擢 升 为 总 主 教 ， 同 年 年 底 被 擢 升 为 枢 机 ； 2009 年 2 月 离 世 。 范 廷 颂 于 1919 年 6 月 15 日 在 越 南 宁 平 省 天 主 教 发 艳 教 区 出 生 ； 童 年 时 接 受 良 好 教 育 后 ， 被 一 位 越 南 神 父 带 到 河 内 继 续 其 学 业 。 范 廷 颂 于 1940 年 在 河 内 大 修 道 院 完 成 神 学 学 业 。 范 廷 颂 于 1949 年 6 月 6 日 在 河 内 的 主 教 座 堂 晋 铎 ； 及 后 被 派 到 圣 女 小 德 兰 孤 儿 院 服 务 。 1950 年 代 ， 范 廷 颂 在 河 内 堂 区 创 建 移 民 接 待 中 心 以 收 容 到 河 内 避 战 的 难 民 。 1954 年 ， 法 越 战 争 结 束 ， 越 南 民 主 共 和 国 建 都 河 内 ， 当 时 很 多 天 主 教 神 职 人 员 逃 至 越 南 的 南 方 ， 但 范 廷 颂 仍 然 留 在 河 内 。 翌 年 管 理 圣 若 望 小 修 院 ； 惟 在 1960 年 因 捍 卫 修 院 的 自 由 、 自 治 及 拒 绝 政 府 在 修 院 设 政 治 课 的 要 求 而 被 捕 。 1963 年 4 月 5 日 ， 教 宗 任 命 范 廷 颂 为 天 主 教 北 宁 教 区 主 教 ， 同 年 8 月 15 日 就 任 ； 其 牧 铭 为 「 我 信 天 主 的 爱 」 。 由 于 范 廷 颂 被 越 南 政 府 软 禁 差 不 多 30 年 ， 因 此 他 无 法 到 所 属 堂 区 进 行 牧 灵 工 作 而 专 注 研 读 等 工 作 。 范 廷 颂 除 了 面 对 战 争 、 贫 困 、 被 当 局 迫 害 天 主 教 会 等 问 题 外 ， 也 秘 密 恢 复 修 院 、 创 建 女 修 会 团 体 等 。 1990 年 ， 教 宗 若 望 保 禄 二 世 在 同 年 6 月 18 日 擢 升 范 廷 颂 为 天 主 教 河 内 总 教 区 宗 座 署 [SEP]\n"
     ]
    }
   ],
   "source": [
    "# 问题 + 上下文 的编码形式：\n",
    "question_context_encode = tokenizer(tran_sample['data'][0]['paragraphs'][0]['qas'][0]['question'],\n",
    "                                    tran_sample['data'][0]['paragraphs'][0]['context'],\n",
    "                                    return_tensors=\"pt\",\n",
    "                                    padding=True,\n",
    "                                    truncation=True,\n",
    "                                    max_length=512\n",
    "                                   )[\"input_ids\"][0]\n",
    "print(question_context_encode)\n",
    "print(tokenizer.decode(question_context_encode))\n",
    "\n",
    "# 发现数字没有拆开，这对后面的计算起始结束位置带来了影响"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2105131d-5252-409b-a03d-24949cefe9b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10142, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>answer_start</th>\n",
       "      <th>answer_end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>范廷颂枢机（，），圣名保禄·若瑟（），是越南罗马天主教枢机。1963年被任为主教；1990年...</td>\n",
       "      <td>范廷颂是什么时候被任为主教的？</td>\n",
       "      <td>1963年</td>\n",
       "      <td>30</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>范廷颂枢机（，），圣名保禄·若瑟（），是越南罗马天主教枢机。1963年被任为主教；1990年...</td>\n",
       "      <td>1990年，范廷颂担任什么职务？</td>\n",
       "      <td>1990年被擢升为天主教河内总教区宗座署理</td>\n",
       "      <td>41</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>范廷颂枢机（，），圣名保禄·若瑟（），是越南罗马天主教枢机。1963年被任为主教；1990年...</td>\n",
       "      <td>范廷颂是于何时何地出生的？</td>\n",
       "      <td>范廷颂于1919年6月15日在越南宁平省天主教发艳教区出生</td>\n",
       "      <td>97</td>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>月15日在越南宁平省天主教发艳教区出生；童年时接受良好教育后，被一位越南神父带到河内继续其学...</td>\n",
       "      <td>1994年3月，范廷颂担任什么职务？</td>\n",
       "      <td>1994年3月23日，范廷颂被教宗若望保禄二世擢升为天主教河内总教区总主教并兼天主教谅山教区...</td>\n",
       "      <td>441</td>\n",
       "      <td>491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>多天主教神职人员逃至越南的南方，但范廷颂仍然留在河内。翌年管理圣若望小修院；惟在1960年因...</td>\n",
       "      <td>范廷颂是何时去世的？</td>\n",
       "      <td>范廷颂于2009年2月22日清晨在河内离世</td>\n",
       "      <td>478</td>\n",
       "      <td>499</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             context            question  \\\n",
       "0  范廷颂枢机（，），圣名保禄·若瑟（），是越南罗马天主教枢机。1963年被任为主教；1990年...     范廷颂是什么时候被任为主教的？   \n",
       "1  范廷颂枢机（，），圣名保禄·若瑟（），是越南罗马天主教枢机。1963年被任为主教；1990年...    1990年，范廷颂担任什么职务？   \n",
       "2  范廷颂枢机（，），圣名保禄·若瑟（），是越南罗马天主教枢机。1963年被任为主教；1990年...       范廷颂是于何时何地出生的？   \n",
       "3  月15日在越南宁平省天主教发艳教区出生；童年时接受良好教育后，被一位越南神父带到河内继续其学...  1994年3月，范廷颂担任什么职务？   \n",
       "4  多天主教神职人员逃至越南的南方，但范廷颂仍然留在河内。翌年管理圣若望小修院；惟在1960年因...          范廷颂是何时去世的？   \n",
       "\n",
       "                                              answer  answer_start  answer_end  \n",
       "0                                              1963年            30          35  \n",
       "1                              1990年被擢升为天主教河内总教区宗座署理            41          62  \n",
       "2                      范廷颂于1919年6月15日在越南宁平省天主教发艳教区出生            97         126  \n",
       "3  1994年3月23日，范廷颂被教宗若望保禄二世擢升为天主教河内总教区总主教并兼天主教谅山教区...           441         491  \n",
       "4                              范廷颂于2009年2月22日清晨在河内离世           478         499  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_sample(sample_data):\n",
    "    \"\"\"\n",
    "    构建样本，数据打平\n",
    "    \"\"\"\n",
    "    id_arr, title_arr, paragraph_id_arr, context_arr, qa_id, question_arr, answer_arr, answer_start_arr = [], [], [], [], [], [], [], []\n",
    "    for sample in sample_data['data']:\n",
    "        for paragraph in sample['paragraphs']:\n",
    "            for qa in paragraph['qas']:\n",
    "                for answer in qa['answers']:\n",
    "                    id_arr.append(sample['id'])\n",
    "                    title_arr.append(sample['title'])\n",
    "                    paragraph_id_arr.append(paragraph['id'])\n",
    "                    context_arr.append(paragraph['context'])\n",
    "                    qa_id.append(qa['id'])\n",
    "                    question_arr.append(qa['question'])\n",
    "                    answer_arr.append(answer['text'])\n",
    "                    answer_start_arr.append(answer['answer_start'])\n",
    "        # break  # TODO 数据量太多，可以只试试一条\n",
    "    return id_arr, title_arr, paragraph_id_arr, context_arr, qa_id, question_arr, answer_arr, answer_start_arr\n",
    "\n",
    "\n",
    "def rebuild_sample(sample_data, max_length=512):\n",
    "    \"\"\"\n",
    "    使用滑动窗口，重构样本\n",
    "    \"\"\"\n",
    "    re_context_arr, re_question_arr, re_answer_arr, re_answer_start_arr, re_answer_end_arr = [], [], [], [], []\n",
    "    \n",
    "    # 1. 打平样本\n",
    "    _, _, _, context_arr, _, question_arr, answer_arr, answer_start_arr = build_sample(sample_data)\n",
    "\n",
    "    # 2. 根据长度重构样本\n",
    "    space_character_length = 3\n",
    "    for i in range(len(context_arr)):\n",
    "        lenth = len(question_arr[i]) + len(context_arr[i]) + space_character_length\n",
    "        answer_end = answer_start_arr[i] + len(answer_arr[i])  # 答案在上下文的结束下标\n",
    "        # 2.1 问题+上下文+间隔符 的长度 <= 模型输入长度，则无法滑动\n",
    "        if lenth <= max_length:\n",
    "            re_context_arr.append(context_arr[i])\n",
    "            re_question_arr.append(question_arr[i])\n",
    "            re_answer_arr.append(answer_arr[i])\n",
    "            re_answer_start_arr.append(answer_start_arr[i])\n",
    "            re_answer_end_arr.append(answer_end)\n",
    "        # 2.2 问题+上下文+间隔符 的长度 > 模型输入长度，则滑动\n",
    "        else:\n",
    "            context_window_max_length = max_length - len(question_arr[i]) - space_character_length  # 滑动窗口中上下文最大可用长度\n",
    "            for window_start in range(len(context_arr[i])):  # 滑动下标起始位置\n",
    "                window_end = window_start + context_window_max_length  # 滑动下标结束位置\n",
    "                # window_end = window_end if window_end <= len(context_arr[i]) else len(context_arr[i])  # 结束位置不能超过上下文的长度\n",
    "                if window_end > len(context_arr[i]):  # 防止滑动后样本数据暴增，当结束位置超过上下文时，不再滑动\n",
    "                    break\n",
    "                \n",
    "                re_answer_start = answer_start_arr[i] - window_start  # 窗口中，答案的起始位置\n",
    "                re_answer_end = answer_end - window_start  # 窗口中，答案的结束位置\n",
    "\n",
    "                # 如果答案在窗口中\n",
    "                re_context_arr.append(context_arr[i][window_start:window_end])\n",
    "                re_question_arr.append(question_arr[i])\n",
    "                re_answer_arr.append(answer_arr[i])\n",
    "                if 0 <= re_answer_start <= context_window_max_length and 0 <= re_answer_end <= context_window_max_length:\n",
    "                    re_answer_start_arr.append(re_answer_start)\n",
    "                    re_answer_end_arr.append(re_answer_end)\n",
    "                # 如果答案不在窗口中\n",
    "                else:\n",
    "                    re_answer_start_arr.append(0)\n",
    "                    re_answer_end_arr.append(0)\n",
    "\n",
    "    data = {'context': re_context_arr, 'question': re_question_arr, 'answer': re_answer_arr, 'answer_start': re_answer_start_arr, 'answer_end': re_answer_end_arr}\n",
    "    return pd.DataFrame(data, columns=['context', 'question', 'answer', 'answer_start', 'answer_end'])\n",
    "\n",
    "\n",
    "def rebuild_sample2(sample_data, max_length=512):\n",
    "    \"\"\"\n",
    "    不使用滑动窗口，重构样本。\n",
    "    发现使用滑动窗口，服务器内存撑不住\n",
    "    \"\"\"\n",
    "    re_context_arr, re_question_arr, re_answer_arr, re_answer_start_arr, re_answer_end_arr = [], [], [], [], []\n",
    "    \n",
    "    # 1. 打平样本\n",
    "    _, _, _, context_arr, _, question_arr, answer_arr, answer_start_arr = build_sample(sample_data)\n",
    "\n",
    "    # 2. 根据长度重构样本。保证截断后的上下文中有答案\n",
    "    space_character_length = 3\n",
    "    for i in range(len(context_arr)):\n",
    "        lenth = len(question_arr[i]) + len(context_arr[i]) + space_character_length\n",
    "        answer_end = answer_start_arr[i] + len(answer_arr[i])  # 答案在上下文的结束下标\n",
    "        # 2.1 问题+上下文+间隔符 的长度 <= 模型输入长度，无需截断\n",
    "        if lenth <= max_length:\n",
    "            re_context_arr.append(context_arr[i])\n",
    "            re_question_arr.append(question_arr[i])\n",
    "            re_answer_arr.append(answer_arr[i])\n",
    "            re_answer_start_arr.append(answer_start_arr[i])\n",
    "            re_answer_end_arr.append(answer_end)\n",
    "        # 2.2 问题+上下文+间隔符 的长度 > 模型输入长度，则截断上下文\n",
    "        else:\n",
    "            context_window_max_length = max_length - len(question_arr[i]) - space_character_length  # 上下文最大可用长度\n",
    "            for window_start in range(len(context_arr[i])):  # 滑动下标起始位置\n",
    "                window_end = window_start + context_window_max_length  # 滑动下标结束位置\n",
    "                window_end = window_end if window_end <= len(context_arr[i]) else len(context_arr[i])  # 结束位置不能超过上下文的长度\n",
    "                \n",
    "                re_answer_start = answer_start_arr[i] - window_start  # 窗口中，答案的起始位置\n",
    "                re_answer_end = answer_end - window_start  # 窗口中，答案的结束位置\n",
    "                \n",
    "                # 如果答案在窗口中，则截断\n",
    "                if 0 <= re_answer_start <= context_window_max_length and 0 <= re_answer_end <= context_window_max_length:\n",
    "                    re_context_arr.append(context_arr[i][window_start:window_end])\n",
    "                    re_question_arr.append(question_arr[i])\n",
    "                    re_answer_arr.append(answer_arr[i])\n",
    "                    re_answer_start_arr.append(re_answer_start)\n",
    "                    re_answer_end_arr.append(re_answer_end)\n",
    "                    break  # 只要一个\n",
    "    data = {'context': re_context_arr, 'question': re_question_arr, 'answer': re_answer_arr, 'answer_start': re_answer_start_arr, 'answer_end': re_answer_end_arr}\n",
    "    return pd.DataFrame(data, columns=['context', 'question', 'answer', 'answer_start', 'answer_end'])\n",
    "\n",
    "\n",
    "# 注意，下标从 0 开始\n",
    "# build_sample(tran_sample)\n",
    "df_tran_sample = rebuild_sample2(tran_sample)\n",
    "print(df_tran_sample.shape)\n",
    "df_tran_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8265028e-0c3f-4da4-8b0c-ddabc136d2e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 注意，如果直接编码，\"1990\"会被拆成一个 token，需要加入数字保证拆成四个 token\n",
    "# tokenizer.add_tokens(new_tokens=['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'])\n",
    "\n",
    "# 测试\n",
    "# question_context_encode = tokenizer(df_tran_sample['context'][0],\n",
    "#                                     df_tran_sample['question'][0],\n",
    "#                                     return_tensors=\"pt\",\n",
    "#                                     padding=True,\n",
    "#                                     truncation=True,\n",
    "#                                     max_length=512\n",
    "#                                    )[\"input_ids\"][0]\n",
    "# print(question_context_encode)\n",
    "# print(tokenizer.decode(question_context_encode))  # 发现: 年份被中的数字被拆开\n",
    "\n",
    "train_encodings = tokenizer(\n",
    "    df_tran_sample['context'].values.tolist(),\n",
    "    df_tran_sample['question'].values.tolist(),\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=512,\n",
    ")\n",
    "train_encodings.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ff30ac9-c5a1-412b-ad72-780e4d990cfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10142, 512])\n",
      "torch.Size([10142, 512])\n",
      "torch.Size([10142, 512])\n"
     ]
    }
   ],
   "source": [
    "print(train_encodings[\"input_ids\"].shape)\n",
    "print(train_encodings[\"token_type_ids\"].shape)\n",
    "print(train_encodings[\"attention_mask\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc7a5ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings[\"start_positions\"] = torch.tensor(\n",
    "    [\n",
    "        train_encodings.char_to_token(idx, x)\n",
    "        if train_encodings.char_to_token(idx, x) != None\n",
    "        else -1\n",
    "        for idx, x in enumerate(df_tran_sample['answer_start'].values.tolist())\n",
    "    ]\n",
    ")\n",
    "train_encodings[\"end_positions\"] = torch.tensor(\n",
    "    [\n",
    "        train_encodings.char_to_token(idx, x - 1)\n",
    "        if train_encodings.char_to_token(idx, x - 1) != None\n",
    "        else -1\n",
    "        for idx, x in enumerate(df_tran_sample['answer_end'].values.tolist())\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0219c7f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "上下文:  范廷颂枢机（，），圣名保禄·若瑟（），是越南罗马天主教枢机。1963年被任为主教；1990年被擢升为天主教河内总教区宗座署理；1994年被擢升为总主教，同年年底被擢升为枢机；2009年2月离世。范廷颂于1919年6月15日在越南宁平省天主教发艳教区出生；童年时接受良好教育后，被一位越南神父带到河内继续其学业。范廷颂于1940年在河内大修道院完成神学学业。范廷颂于1949年6月6日在河内的主教座堂晋铎；及后被派到圣女小德兰孤儿院服务。1950年代，范廷颂在河内堂区创建移民接待中心以收容到河内避战的难民。1954年，法越战争结束，越南民主共和国建都河内，当时很多天主教神职人员逃至越南的南方，但范廷颂仍然留在河内。翌年管理圣若望小修院；惟在1960年因捍卫修院的自由、自治及拒绝政府在修院设政治课的要求而被捕。1963年4月5日，教宗任命范廷颂为天主教北宁教区主教，同年8月15日就任；其牧铭为「我信天主的爱」。由于范廷颂被越南政府软禁差不多30年，因此他无法到所属堂区进行牧灵工作而专注研读等工作。范廷颂除了面对战争、贫困、被当局迫害天主教会等问题外，也秘密恢复修院、创建女修会团体等\n",
      "\n",
      "问题:  范廷颂是什么时候被任为主教的？\n",
      "解答: 1963 年\n"
     ]
    }
   ],
   "source": [
    "# 看第一个问题的解答\n",
    "print('上下文: ', df_tran_sample['context'][0])\n",
    "print('\\n问题: ', df_tran_sample['question'][0])\n",
    "start_idx, end_idx = train_encodings[\"start_positions\"][0], train_encodings[\"end_positions\"][0]+1\n",
    "print('解答:', tokenizer.decode(train_encodings[\"input_ids\"][0][start_idx : end_idx]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2372e427-6817-41d9-986c-add0ffead641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载训练数据\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "class SquadDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {k: v[idx].to(device) for k, v in self.encodings.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)\n",
    "\n",
    "train_dataset = SquadDataset(train_encodings)\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6ec98d-de1c-427e-8218-1c8bf190755c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a069c3dd-f62d-4600-9f68-dd2db021a758",
   "metadata": {},
   "source": [
    "## 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ad8490e-e398-4588-b6bf-f83823f4ca15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start time: 2024-04-05 12:25:23.690488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 2536/2536 [22:14<00:00,  1.90it/s, Epoch 0, train loss: 2.0593, acc start: 0.4456, acc end: 0.4662]\n",
      "Epoch 1: 100%|██████████| 2536/2536 [22:15<00:00,  1.90it/s, Epoch 1, train loss: 1.3067, acc start: 0.5902, acc end: 0.6293]\n",
      "Epoch 2: 100%|██████████| 2536/2536 [22:15<00:00,  1.90it/s, Epoch 2, train loss: 0.9728, acc start: 0.6750, acc end: 0.7000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End time: 2024-04-05 13:32:09.031793\n",
      "Consume time of second: 4005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import time, datetime\n",
    "from torch.optim import AdamW\n",
    "\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "print('Start time:', start_time)\n",
    "\n",
    "# 训练\n",
    "optim = AdamW(model.parameters(), lr=5e-5)\n",
    "model.train()\n",
    "for epoch in range(3):\n",
    "    loss_sum = 0.0\n",
    "    acc_start_sum = 0.0\n",
    "    acc_end_sum = 0.0\n",
    "    pbar = tqdm(train_loader, desc=f'Epoch {epoch}')\n",
    "    for batch_idx, batch in enumerate(pbar):\n",
    "        optim.zero_grad()\n",
    "\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        start_positions = batch[\"start_positions\"]\n",
    "        end_positions = batch[\"end_positions\"]\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            start_positions=start_positions,\n",
    "            end_positions=end_positions,\n",
    "        )\n",
    "        # 需要注意的是，由于损失值是在多个 GPU 上进行计算的，因此得到的 `loss.out` 是一个向量，而不是一个标量。我们可以用 .mean() 对多块 GPU 上的 loss 值求均值，将其转换为一个标量\n",
    "        loss = outputs.loss.mean()\n",
    "        # if fp16_training:\n",
    "        #     accelerator.backward(loss)\n",
    "        # else:\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        loss_sum += loss.item()\n",
    "\n",
    "        ### START CODE HERE ###\n",
    "        # Obtain answer by choosing the most probable start position / end position\n",
    "        # Using `torch.argmax` and its `dim` parameter to extract preditions for start position and end position.\n",
    "        start_pred = torch.argmax(outputs.start_logits, dim=1)\n",
    "        end_pred = torch.argmax(outputs.end_logits, dim=1)\n",
    "\n",
    "        # calculate accuracy for start and end positions. eg., using start_pred and start_positions to calculate acc_start.\n",
    "        acc_start = (start_pred == start_positions).float().mean()\n",
    "        acc_end = (end_pred == end_positions).float().mean()\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        acc_start_sum += acc_start\n",
    "        acc_end_sum += acc_end\n",
    "\n",
    "        # Update progress bar\n",
    "        postfix = {\n",
    "            \"loss\": f\"{loss_sum/(batch_idx+1):.4f}\",\n",
    "            \"acc_start\": f\"{acc_start_sum/(batch_idx+1):.4f}\",\n",
    "            \"acc_end\": f\"{acc_end_sum/(batch_idx+1):.4f}\",\n",
    "        }\n",
    "\n",
    "        # Add batch accuracy to progress bar\n",
    "        batch_desc = f\"Epoch {epoch}, train loss: {postfix['loss']}\"\n",
    "        pbar.set_postfix_str(\n",
    "            f\"{batch_desc}, acc start: {postfix['acc_start']}, acc end: {postfix['acc_end']}\"\n",
    "        )\n",
    "\n",
    " \n",
    "end_time = datetime.datetime.now()\n",
    "print('End time:', end_time)\n",
    "\n",
    "consume_time = end_time - start_time\n",
    "print('Consume time of second:', consume_time.seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5857f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "89dfbf2a-04c5-4555-aeb7-0590555ea92a",
   "metadata": {},
   "source": [
    "## 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "04fb9266-dcb7-404e-af57-d1067ba7f9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个名为 predict 的函数，接收两个参数 doc 和 query\n",
    "def predict(doc, query):\n",
    "    # 输出“段落：”和  doc 的内容\n",
    "    print(\"段落：\", doc)\n",
    "    # 输出“提问：”和 query 的内容\n",
    "    print(\"提问：\", query)\n",
    "    # 将 doc 和 query 传递给 tokenizer 函数，将返回结果赋值给 item\n",
    "    item = tokenizer(\n",
    "        [doc, query], max_length=512, return_tensors=\"pt\", truncation=True, padding=True\n",
    "    )\n",
    "    # 关闭 torch 的梯度计算\n",
    "    with torch.no_grad():\n",
    "        # 将 input_ids 和 attention_mask 传递给 model，将返回结果赋值给 outputs\n",
    "        input_ids = item[\"input_ids\"].to(device).reshape(1, -1)\n",
    "        attention_mask = item[\"attention_mask\"].to(device).reshape(1, -1)\n",
    "\n",
    "        outputs = model(input_ids[:, :512], attention_mask[:, :512])\n",
    "\n",
    "        ### START CODE HERE ###\n",
    "        # 使用`torch.argmax`和它的`dim`参数来提取开始位置和结束位置的预测结果\n",
    "        start_pred = torch.argmax(outputs.start_logits, dim=1)\n",
    "        end_pred = torch.argmax(outputs.end_logits, dim=1)\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "    # 将预测结果转为字符级别\n",
    "    try:\n",
    "        start_pred = item.token_to_chars(0, start_pred)\n",
    "        end_pred = item.token_to_chars(0, end_pred)\n",
    "    except:\n",
    "        # 如果出现异常，则返回“无法找到答案”\n",
    "        return \"无法找到答案\"\n",
    "\n",
    "    # 判断结果是否有效，如果有效则返回结果\n",
    "    if start_pred.start > end_pred.end:\n",
    "        # 如果预测的开始位置大于结束位置，则返回“无法找到答案”\n",
    "        return \"无法找到答案\"\n",
    "    else:\n",
    "        # 如果预测的开始位置小于结束位置，则返回预测的答案\n",
    "        return doc[start_pred.start : end_pred.end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f6ca6c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "段落： 《战国无双3》（）是由光荣和ω-force开发的战国无双系列的正统第三续作。本作以三大故事为主轴，分别是以武田信玄等人为主的《关东三国志》，织田信长等人为主的《战国三杰》，石田三成等人为主的《关原的年轻武者》，丰富游戏内的剧情。此部份专门介绍角色，欲知武器情报、奥义字或擅长攻击类型等，请至战国无双系列1.由于乡里大辅先生因故去世，不得不寻找其他声优接手。从猛将传 and Z开始。2.战国无双 编年史的原创男女主角亦有专属声优。此模式是任天堂游戏谜之村雨城改编的新增模式。本作中共有20张战场地图（不含村雨城），后来发行的猛将传再新增3张战场地图。但游戏内战役数量繁多，部分地图会有兼用的状况，战役虚实则是以光荣发行的2本「战国无双3 人物真书」内容为主，以下是相关介绍。（注：前方加☆者为猛将传新增关卡及地图。）合并本篇和猛将传的内容，村雨城模式剔除，战国史模式可直接游玩。主打两大模式「战史演武」&「争霸演武」。系列作品外传作品\n",
      "提问： 《战国无双3》是由哪两个公司合作开发的？\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'光荣和ω-force'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "# 在 dev 数据上进行检验\n",
    "with open(dev_file_path, 'r+', encoding='utf-8') as dev_file:\n",
    "     dev_sample = json.load(dev_file)\n",
    "df_dev_sample = rebuild_sample2(dev_sample)\n",
    "predict(\n",
    "    df_dev_sample[\"context\"][0],\n",
    "    df_dev_sample[\"question\"][0],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "11976de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "段落： 勒布朗·詹姆斯是一位美国职业篮球运动员，现效力于洛杉矶湖人队。他身高 203 厘米，体重 113 公斤，司职小前锋/大前锋。詹姆斯出生于俄亥俄州阿克伦市，高中时期便展现出了惊人的篮球天赋，成为高中时期最受瞩目的篮球选手之一。2003 年，他以状元秀的身份进入 NBA 并加入克利夫兰骑士队。在骑士队期间，詹姆斯多次带领球队进入季后赛，并在 2016 年带领球队获得总冠军头衔。此后，他先后效力于迈阿密热火队和克利夫兰骑士队，均取得了显著的成绩。詹姆斯是一个多面手球员，擅长得分、助攻、篮板等多项技术，并在场上表现出很高的篮球智商。他也是一名全方位的球员，场上表现出极强的威慑力，常常成为对手眼中的难题。詹姆斯不仅在 NBA 赛场上成绩斐然，他也是体坛巨星，被誉为篮球界最伟大的球员之一。\n",
      "提问： 詹姆斯在哪个球队？\n",
      "洛杉矶湖人队\n"
     ]
    }
   ],
   "source": [
    "# 自定义数据进行检验\n",
    "doc = \"勒布朗·詹姆斯是一位美国职业篮球运动员，现效力于洛杉矶湖人队。他身高 203 厘米，体重 113 公斤，司职小前锋/大前锋。詹姆斯出生于俄亥俄州阿克伦市，高中时期便展现出了惊人的篮球天赋，成为高中时期最受瞩目的篮球选手之一。2003 年，他以状元秀的身份进入 NBA 并加入克利夫兰骑士队。在骑士队期间，詹姆斯多次带领球队进入季后赛，并在 2016 年带领球队获得总冠军头衔。此后，他先后效力于迈阿密热火队和克利夫兰骑士队，均取得了显著的成绩。詹姆斯是一个多面手球员，擅长得分、助攻、篮板等多项技术，并在场上表现出很高的篮球智商。他也是一名全方位的球员，场上表现出极强的威慑力，常常成为对手眼中的难题。詹姆斯不仅在 NBA 赛场上成绩斐然，他也是体坛巨星，被誉为篮球界最伟大的球员之一。\"\n",
    "\n",
    "print(predict(doc=doc, query=\"詹姆斯在哪个球队？\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cd872314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "段落： 勒布朗·詹姆斯是一位美国职业篮球运动员，现效力于洛杉矶湖人队。他身高 203 厘米，体重 113 公斤，司职小前锋/大前锋。詹姆斯出生于俄亥俄州阿克伦市，高中时期便展现出了惊人的篮球天赋，成为高中时期最受瞩目的篮球选手之一。2003 年，他以状元秀的身份进入 NBA 并加入克利夫兰骑士队。在骑士队期间，詹姆斯多次带领球队进入季后赛，并在 2016 年带领球队获得总冠军头衔。此后，他先后效力于迈阿密热火队和克利夫兰骑士队，均取得了显著的成绩。詹姆斯是一个多面手球员，擅长得分、助攻、篮板等多项技术，并在场上表现出很高的篮球智商。他也是一名全方位的球员，场上表现出极强的威慑力，常常成为对手眼中的难题。詹姆斯不仅在 NBA 赛场上成绩斐然，他也是体坛巨星，被誉为篮球界最伟大的球员之一。\n",
      "提问： 詹姆斯出生于哪个城市？\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'俄亥俄州阿克伦市'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(doc=doc, query=\"詹姆斯出生于哪个城市？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "12178582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "段落： 对于期货资产，投资者既可以做多也可以做空，因此\"上涨趋势\"和\"下跌趋势\"都可以作为交易信号。我们将平均最大回撤和平均最大反向回撤中较小的那一个，作为市场情绪平稳度指标。市场情绪平稳度指标越小，则上涨或者下跌的趋势越强。然后我们再根据具体是上涨还是下跌的趋势，即可判断交易方向。\n",
      "提问： 什么资产既可以做多也可以做空？\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'对于期货资产'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(\n",
    "    doc='对于期货资产，投资者既可以做多也可以做空，因此\"上涨趋势\"和\"下跌趋势\"都可以作为交易信号。我们将平均最大回撤和平均最大反向回撤中较小的那一个，作为市场情绪平稳度指标。市场情绪平稳度指标越小，则上涨或者下跌的趋势越强。然后我们再根据具体是上涨还是下跌的趋势，即可判断交易方向。',\n",
    "    query=\"什么资产既可以做多也可以做空？\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2591433",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d89b23e4-3d43-4bcd-96fc-2c22fa9f556d",
   "metadata": {},
   "source": [
    "## 保存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a5a462cc-8f2a-4a6b-9df6-db6029dff52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '/root/workspace/model/model_bert-base-chinese/qa.pt'\n",
    "# 对于单 GPU 训练的模型，直接用 .save_pretrained()\n",
    "model.save_pretrained(model_path, from_pt=True)\n",
    "# 对于多 GPU 训练得到的模型，要加上 .module\n",
    "# model.module.save_pretrained(model_path, from_pt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4941a7-995c-4250-917f-b183c0786112",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "779b6d6f",
   "metadata": {},
   "source": [
    "## 加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1fff1a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = BertForQuestionAnswering.from_pretrained(model_path).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afaeeac6-b2ad-46dd-ad38-a2c105f87bce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267f8ba3-7c5a-4baf-aab7-ba41215706e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d83da05-065c-4d90-b9b0-eb6db99042be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3930ccec-e709-4487-9f7b-1a6fe906748b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
